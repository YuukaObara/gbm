{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuukaObara/gbm/blob/main/%E6%B0%91%E6%B3%8A%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%AE%E5%AE%BF%E6%B3%8A%E4%BE%A1%E6%A0%BC%E4%BA%88%E6%B8%AC%EF%BC%88Python%EF%BC%89_%E5%B0%8F%E5%8E%9F%E5%84%AA%E4%BD%B3_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **データの読み込み**"
      ],
      "metadata": {
        "id": "2c6HX3FeU7yU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ESWrzsPFOr5"
      },
      "outputs": [],
      "source": [
        "#ドライブと接続する\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuHYIrLI9-Io"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "!pip install ydata-profiling\n",
        "!pip install sweetviz\n",
        "import ydata_profiling\n",
        "!pip install numpy==1.24.3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import phik\n",
        "import sweetviz as sv\n",
        "from sklearn.metrics import r2_score\n",
        "#Count Encoding\n",
        "!pip install category_encoders\n",
        "from category_encoders import OrdinalEncoder, CountEncoder, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "#optuna\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXdj1PqjFMUq"
      },
      "outputs": [],
      "source": [
        "#【練習問題】民泊サービスの宿泊価格予測(https://signate.jp/competitions/266)のデータ使用\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/民泊サービスの宿泊価格予測/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/民泊サービスの宿泊価格予測/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **データの可視化**"
      ],
      "metadata": {
        "id": "WkVPhd3iUY8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "QcskTHgrlWTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "DL6Lgdh0q1nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 施設数：都市別・施設規模別\n",
        "# accommodatesを1, 2, 3以上に分けるための新しいカラムを作成\n",
        "df_train['accommodates_group'] = pd.cut(df_train['accommodates'], bins=[0, 1, 2, float('inf')], labels=['1', '2', '3+'])\n",
        "\n",
        "# cityと新しいカラムでグループ化し、数をカウント\n",
        "accommodates_city_counts = df_train.groupby(['city', 'accommodates_group'])['accommodates_group'].count().unstack(fill_value=0)\n",
        "\n",
        "# 結果を表示\n",
        "accommodates_city_counts"
      ],
      "metadata": {
        "id": "JQ4vv_mP-oPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 施設数（％表示）：都市別・施設規模別\n",
        "# 各cityの合計値を計算\n",
        "city_totals = accommodates_city_counts.sum(axis=1)\n",
        "\n",
        "# 各グループの値をcityの合計値で割ってパーセンテージを計算\n",
        "accommodates_city_percentage = accommodates_city_counts.div(city_totals, axis=0) * 100\n",
        "\n",
        "# 結果を表示\n",
        "accommodates_city_percentage.round(1)"
      ],
      "metadata": {
        "id": "dPc_eHnh_wqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 施設数（％表示）円グラフ：都市別・施設規模別\n",
        "for city in accommodates_city_percentage.index:\n",
        "    plt.figure(figsize=(3, 3))  # グラフのサイズを設定\n",
        "    plt.pie(accommodates_city_percentage.loc[city],\n",
        "            labels=accommodates_city_percentage.columns,\n",
        "            autopct='%1.1f%%',  # パーセンテージの表示形式\n",
        "            startangle=90)  # グラフの開始角度\n",
        "    plt.title(f'{city}')  # グラフのタイトル\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NeHk5IJFCtAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 平均宿泊価格：都市別・施設規模別\n",
        "# cityとaccommodates_groupでグループ化し、yの平均を計算\n",
        "city_accommodates_y_mean = df_train.groupby(['city', 'accommodates_group'])['y'].mean().unstack()\n",
        "\n",
        "# 全都市の平均を計算\n",
        "all_city_mean = df_train.groupby('accommodates_group')['y'].mean()\n",
        "\n",
        "# 全都市の平均を新しい行として追加\n",
        "city_accommodates_y_mean.loc['All Cities'] = all_city_mean\n",
        "\n",
        "# 結果を表示\n",
        "city_accommodates_y_mean.round(1)"
      ],
      "metadata": {
        "id": "ROTsoSUWsvRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 宿泊価格中央値：都市別・施設規模別\n",
        "# cityとaccommodates_groupでグループ化し、yの中央値を計算\n",
        "city_accommodates_y_median = df_train.groupby(['city', 'accommodates_group'])['y'].median().unstack()\n",
        "\n",
        "# 全都市の平均を計算\n",
        "all_city_median = df_train.groupby('accommodates_group')['y'].median()\n",
        "\n",
        "# 全都市の平均を新しい行として追加\n",
        "city_accommodates_y_median.loc['All Cities'] = all_city_median\n",
        "\n",
        "# 結果を表示\n",
        "city_accommodates_y_median.round(1)"
      ],
      "metadata": {
        "id": "3xwRqwzvumlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 施設数：都市別\n",
        "df_train['city'].value_counts()"
      ],
      "metadata": {
        "id": "qi4pE_JJ_UvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 宿泊価格の平均・中央値：都市別\n",
        "# cityの値ごとのyの平均と中央値を計算\n",
        "city_stats = df_train.groupby('city')['y'].agg(['mean', 'median'])\n",
        "# 中央値で降順にソート\n",
        "city_stats = city_stats.sort_values('median', ascending=False)\n",
        "\n",
        "# 折れ線グラフで表示\n",
        "plt.figure(figsize=(8, 5))  # グラフのサイズを設定\n",
        "plt.plot(city_stats.index, city_stats['mean'], marker='o', label='mean')\n",
        "plt.plot(city_stats.index, city_stats['median'], marker='x', label='median')\n",
        "plt.xlabel('city')  # x軸ラベル\n",
        "plt.ylabel('y')  # y軸ラベル\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rtqqC9i3-Z7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#相関係数ヒートマップ　数値型のみ\n",
        "sns.heatmap(data=df_train.select_dtypes(include=['number']).corr().round(2), annot=True, vmax=1, vmin=-1, cmap='bwr')"
      ],
      "metadata": {
        "id": "lVRDCQZ0q4TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **データの変換**"
      ],
      "metadata": {
        "id": "AjFUE9NzjtNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 各都市名のカラムを作成\n",
        "for city in ['NYC', 'LA', 'SF', 'DC', 'Chicago', 'Boston']:\n",
        "    df_train[city] = df_train['city'].str.contains(city)\n",
        "for city in ['NYC', 'LA', 'SF', 'DC', 'Chicago', 'Boston']:\n",
        "    df_test[city] = df_test['city'].str.contains(city)"
      ],
      "metadata": {
        "id": "g7lgvr5GWsrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIt-qbegDV4p"
      },
      "outputs": [],
      "source": [
        "# amenities列内の単語出現回数TOP5を出力\n",
        "from collections import Counter\n",
        "# すべてのamenitiesの値を結合\n",
        "all_amenities = ' '.join(df_train['amenities'].astype(str).tolist())\n",
        "# 単語に分割\n",
        "words = all_amenities.split(',')\n",
        "# 出現回数をカウント\n",
        "word_counts = Counter(words)\n",
        "# 出現頻度の高い順に単語と出現回数を表示\n",
        "for word, count in word_counts.most_common(5):\n",
        "    print(f'{word}: {count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVVj3rsTIUDM"
      },
      "outputs": [],
      "source": [
        "# 各アメニティ名のカラムを作成\n",
        "for amenity in ['Air conditioning', 'Internet', 'TV', 'Free parking on premises', 'Heating', 'Smoke detector', 'Essentials']:\n",
        "    df_train[amenity] = df_train['amenities'].str.contains(amenity)\n",
        "for amenity in ['Air conditioning', 'Internet', 'TV', 'Free parking on premises', 'Heating', 'Smoke detector', 'Essentials']:\n",
        "    df_test[amenity] = df_test['amenities'].str.contains(amenity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsUA9IPu-lVu"
      },
      "outputs": [],
      "source": [
        "# 今回の分析で使わないカラムの削除、数値の変換、エンコーディング、カテゴリ変数をカテゴリ型に変換\n",
        "def data_pre(df): #データの前処理用の関数を作成\n",
        "    #列を削除する id, name, description, neighbourhood(93%が欠損値), host_has_profile_pic(99%以上が同じ値)\n",
        "    df = df.drop(columns = ['id', 'name', 'description', 'thumbnail_url', 'host_has_profile_pic', 'cleaning_fee', 'host_identity_verified', 'cancellation_policy',\n",
        "                            'instant_bookable', 'bed_type', 'zipcode', 'number_of_reviews', 'room_type', 'neighbourhood', 'property_type', 'city', 'beds', 'amenities', 'first_review'])\n",
        "\n",
        "    #host_response_rateをパーセンテージから割合にする\n",
        "    df['host_response_rate'] = df['host_response_rate'].str.replace('%', '').astype(float)/100\n",
        "    #'host_since' を日付型に変換　最新の日付から経過日数を計算し代入\n",
        "    df['host_since'] = pd.to_datetime(df['host_since'])\n",
        "    newest_date = df['host_since'].max()\n",
        "    df['host_since'] = (newest_date - df['host_since']).dt.days\n",
        "    # 'last_review' を日付型に変換　最新の日付から経過日数を計算し代入\n",
        "    df['last_review'] = pd.to_datetime(df['last_review'])\n",
        "    newest_date = df['last_review'].max()\n",
        "    df['last_review'] = (newest_date - df['last_review']).dt.days\n",
        "    # # 'first_review' を日付型に変換　最新の日付から経過日数を計算し代入\n",
        "    # df['first_review'] = pd.to_datetime(df['first_review'])\n",
        "    # newest_date = df['first_review'].max()\n",
        "    # df['first_review'] = (newest_date - df['first_review']).dt.days\n",
        "\n",
        "    #Encoding　カテゴリ型に変換\n",
        "    def encode_categorical(df, cols):\n",
        "        for col in cols:\n",
        "            #Label Encoding\n",
        "            le = LabelEncoder()\n",
        "            not_null = df[col][df[col].notnull()]\n",
        "            df[col] = pd.Series(le.fit_transform(not_null),index = not_null.index)\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "        return df\n",
        "    df = encode_categorical(df, cols = ['NYC', 'LA', 'SF', 'DC', 'Chicago', 'Boston', 'Air conditioning', 'Internet', 'TV', 'Free parking on premises', 'Heating', 'Smoke detector', 'Essentials'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 変換実行\n",
        "df_train_encoded = data_pre(df_train)\n",
        "df_test_encoded = data_pre(df_test)"
      ],
      "metadata": {
        "id": "ByDceIxeHd_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4e5j2pVTeBa"
      },
      "outputs": [],
      "source": [
        "#Count Encoding\n",
        "#cols = ['Air conditioning', 'Internet', 'TV', 'Free parking on premises', 'Heating', 'Smoke detector', 'Essentials']\n",
        "# 各カラムにCount Encodingを適用\n",
        "#def count_encoding(df, cols):\n",
        "#    for col in cols:\n",
        "#        encoder = CountEncoder()  # CountEncoderのインスタンスを作成\n",
        "#        encoder.fit(df[col])  # 学習データでencoderを学習\n",
        "#        # 学習データとテストデータに変換を適用\n",
        "#        df[f'{col}_count'] = encoder.transform(df[col])\n",
        "#        # 元のカラムを削除\n",
        "#        df = df.drop(columns=[col])\n",
        "#    return df\n",
        "#df_train_encoded = count_encoding(df_train_encoded, cols)\n",
        "#df_test_encoded = count_encoding(df_test_encoded, cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTf7D4Lb_ZAC"
      },
      "outputs": [],
      "source": [
        "#One hot Encoding\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "#encoded_data = pd.get_dummies(df_train_encoded['room_type'])\n",
        "#df_train_encoded = pd.concat([df_train_encoded, encoded_data], axis=1)\n",
        "#df_train_encoded = df_train_encoded.drop(columns=['room_type'])\n",
        "#encoded_data = pd.get_dummies(df_test_encoded['room_type'])\n",
        "#df_test_encoded = pd.concat([df_test_encoded, encoded_data], axis=1)\n",
        "#df_test_encoded = df_test_encoded.drop(columns=['room_type'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_encoded"
      ],
      "metadata": {
        "id": "I7mIZE86XXMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ツールによる可視化**"
      ],
      "metadata": {
        "id": "Dh99JDmXoUeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = sv.compare(df_train_encoded, df_test_encoded, target_feat=\"y\")"
      ],
      "metadata": {
        "id": "JW87wrbUHhUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report.show_notebook()"
      ],
      "metadata": {
        "id": "EDdlP4FmHhN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_encoded.profile_report()"
      ],
      "metadata": {
        "id": "6wf-8ZSxVGUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **モデル構築**"
      ],
      "metadata": {
        "id": "bZ0IG8g1VGEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVq8_YAc2wGo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# # LightGBMのモデル構築、チューニング、評価\n",
        "\n",
        "# from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# #訓練データを説明変数と目的変数に分割\n",
        "# col = \"y\"\n",
        "# y = df_train_encoded[col] #yのみを目的変数にする\n",
        "# X = df_train_encoded.drop(col, axis=1) #col以外のカラムを説明変数にする\n",
        "\n",
        "# # データを訓練用とテスト用に分割\n",
        "# # 訓練データでモデルを学習させ、テストデータで性能を確認する\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# def objective(trial):\n",
        "#     # Optunaでチューニングするパラメータを定義\n",
        "#     # trial.suggest_XXXを使って、探索するパラメータの範囲や値を指定\n",
        "#     params = {\n",
        "#         'objective': 'regression',  # 回帰問題を指定\n",
        "#         'metric': 'rmse',  # モデルの性能指標としてRMSE（平均二乗平方根誤差）を使用\n",
        "#         # 'n_estimators': trial.suggest_int('n_estimators', 200, 500),  # 決定木の数（データサイズに合わせ調整）\n",
        "#         'max_depth': trial.suggest_int('max_depth', 3, 5),  # 決定木の深さ（浅めに設定）\n",
        "#         # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),  # 学習率（データサイズに基づき調整）\n",
        "#         'num_leaves': trial.suggest_int('num_leaves', 10, 50),  # 決定木のリーフ数（データ特徴量数に適合）\n",
        "#         # 'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),  # 特徴量のサブサンプル比率\n",
        "#         # 'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),  # データサンプルのサブサンプル比率\n",
        "#         # 'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),  # バギングの頻度（頻度を高めに設定）\n",
        "#         # 'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 1.0, log=True),  # L1正則化の強さ\n",
        "#         # 'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1.0, log=True),  # L2正則化の強さ\n",
        "#         # 'min_child_weight': trial.suggest_float('min_child_weight', 0.01, 1.0, log=True),  # 子ノードで必要な最小データ量\n",
        "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 30)\n",
        "#     }\n",
        "\n",
        "#     # LightGBMのモデルを初期化\n",
        "#     model = lgb.LGBMRegressor(**params, random_state=42)\n",
        "\n",
        "#     # クロスバリデーションを使用してモデルの性能を評価\n",
        "#     # 訓練データをさらに分割し、複数回学習・評価を繰り返して平均スコアを算出\n",
        "#     score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "#     # クロスバリデーションのスコア（負のMSE）の平均値を返す\n",
        "#     # Optunaはこの値を元にパラメータを最適化する\n",
        "#     return np.mean(score)\n",
        "\n",
        "# # Optunaのチューニングを実行\n",
        "# # maximize: 負のMSEの平均値を最大化する（つまり、MSEを最小化することに相当）\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=50)  # 50回の試行で最適化を行う\n",
        "\n",
        "# # 最適なパラメータを取得\n",
        "# best_params = study.best_params\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# # 最適なパラメータでLightGBMモデルを学習\n",
        "# best_model = lgb.LGBMRegressor(**best_params, random_state=42)\n",
        "# best_model.fit(X_train, y_train)  # 訓練データでモデルを学習\n",
        "\n",
        "# # テストデータでの評価\n",
        "# predictions = best_model.predict(X_test)  # 学習したモデルでテストデータを予測\n",
        "# rmse = mean_squared_error(y_test, predictions)  # RMSEを計算\n",
        "# print(\"Test RMSE:\", np.sqrt(rmse))  # テストデータでの性能を表示\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGn_FRfe2wCx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# print(best_params)\n",
        "# print(\"Test RMSE:\", np.sqrt(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hcbnBx92v_J"
      },
      "outputs": [],
      "source": [
        "# チューニングによるベストパラメータを用いて新しいコードに適用、評価\n",
        "\n",
        "# 訓練データを説明変数と目的変数に分割\n",
        "col = \"y\"\n",
        "y = df_train_encoded[col]  # yのみを目的変数にする\n",
        "X = df_train_encoded.drop(col, axis=1)  # col以外のカラムを説明変数にする\n",
        "\n",
        "# 分割数、学習回数、実行メッセージを出さない\n",
        "FOLD = 5\n",
        "NUM_ROUND = 100\n",
        "VERBOSE_EVAL = -1\n",
        "\n",
        "# ベストパラメータを適用\n",
        "# params = best_params # ここでベストパラメータを使用しています\n",
        "params = {}\n",
        "params['objective'] = 'regression'\n",
        "params['metric'] = 'rmse'\n",
        "params['verbose'] = -1\n",
        "params['max_depth'] = 5\n",
        "params['num_leaves'] = 27\n",
        "params['min_data_in_leaf'] = 22\n",
        "\n",
        "valid_scores_rmse = []\n",
        "valid_scores_r2 = []\n",
        "models = []\n",
        "evals_result = {}\n",
        "evals_result_for_learning_curve = []\n",
        "kf = KFold(n_splits=FOLD, shuffle=True, random_state=4)\n",
        "\n",
        "# データの分割と学習\n",
        "for fold, (train_indices, valid_indices) in enumerate(kf.split(X)):\n",
        "    X_train, X_valid = X.iloc[train_indices], X.iloc[valid_indices]\n",
        "    y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_eval = lgb.Dataset(X_valid, y_valid)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_train,\n",
        "        valid_sets=[lgb_train, lgb_eval],\n",
        "        num_boost_round=NUM_ROUND,\n",
        "        callbacks=[lgb.log_evaluation(VERBOSE_EVAL), lgb.record_evaluation(evals_result), lgb.early_stopping(stopping_rounds=100)],\n",
        "    )\n",
        "\n",
        "    y_valid_pred = model.predict(X_valid)\n",
        "\n",
        "    #学習曲線\n",
        "    evals_result_for_learning_curve.append(evals_result.copy())\n",
        "    #RMSE\n",
        "    score_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
        "    print(f'fold {fold} RMSE: {score_rmse}')\n",
        "    valid_scores_rmse.append(score_rmse)\n",
        "    #adjustedR2\n",
        "    n = len(y)\n",
        "    k = X.shape[1]\n",
        "    score_r2 = (1 - (1 - r2_score(y_valid, y_valid_pred)) * (n - 1) / (n - k - 1))\n",
        "    print(f'fold {fold} adjustedR2: {score_r2}')\n",
        "    valid_scores_r2.append(score_r2)\n",
        "\n",
        "    models.append(model)\n",
        "\n",
        "#RMSE出力\n",
        "cv_score = np.mean(valid_scores_rmse)\n",
        "print(f'CV score: {cv_score}')\n",
        "#adjustedR2出力\n",
        "score_r2 = np.mean(valid_scores_r2)\n",
        "print(f'adjustedR2: {score_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **予測値算出**"
      ],
      "metadata": {
        "id": "S4T42lOFos0w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPNBRRxv2v7a"
      },
      "outputs": [],
      "source": [
        "# 予測値算出 全モデルの平均\n",
        "predict = np.mean([model.predict(df_test_encoded) for model in models], axis=0)\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgaFNitqaC-M"
      },
      "outputs": [],
      "source": [
        "#予測値を入れるカラムを作る\n",
        "df_test[\"y\"] = predict\n",
        "#予測値のカラムのみをcsvとして出力\n",
        "df_test[\"y\"].to_csv(\"submit_test.csv\", header = False)\n",
        "from google.colab import files\n",
        "files.download(\"submit_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#予測値表示\n",
        "df_test"
      ],
      "metadata": {
        "id": "Ffd4tpR14cEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **評価指標可視化**"
      ],
      "metadata": {
        "id": "5A_42qKRoqpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_xwxBRKGlk9"
      },
      "outputs": [],
      "source": [
        "# 学習曲線\n",
        "lgb.plot_metric(evals_result, metric='rmse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-2fofgExd_g"
      },
      "outputs": [],
      "source": [
        "# SHAP値\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "\n",
        "# notebook内でJavascriptを動かすために入れる\n",
        "shap.initjs()\n",
        "\n",
        "# TreeExplainerは、決定木系(XGBoost、lightBGM等含む)のモデルのSHAP値を取得するもの。\n",
        "explainer = shap.TreeExplainer(model=models[-1])\n",
        "explainer.expected_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uP93CcEyGi5"
      },
      "outputs": [],
      "source": [
        "# SHAP値の計算\n",
        "X_valid_shap = X_valid.copy().reset_index(drop=True)\n",
        "shap_values = explainer.shap_values(X=X_valid_shap)\n",
        "\n",
        "print(X_valid_shap.shape)\n",
        "print(shap_values.shape)\n",
        "print(shap_values[0])#テストデータの0番目の要素を出力"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 欠損値の数\n",
        "df_train_encoded.isnull().sum()"
      ],
      "metadata": {
        "id": "wPUT0kOvtTYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz4qw6AlyStd"
      },
      "outputs": [],
      "source": [
        "# SHAP サマリープロット\n",
        "shap.summary_plot(shap_values, X_valid_shap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIdJYW0N7bMh"
      },
      "outputs": [],
      "source": [
        "# SHAP サマリープロット（絶対値）\n",
        "shap.summary_plot(shap_values, X_valid_shap, plot_type='bar')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7i7SLdyQyM1VrnbQ5nbCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}